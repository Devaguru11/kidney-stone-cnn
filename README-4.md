# ğŸš€ Kidney Stone Detection â€” Phase 4: FastAPI Inference Server

> **Status:** âœ… Complete  
> **Duration:** ~1 Day  
> **Last Updated:** February 2026  
> **Author:** Devaguru

---

## ğŸ“‹ Phase Overview

Phase 4 wraps the trained EfficientNet-B4 model in a production-ready REST API using FastAPI. The API accepts CT scan images and returns predictions, confidence scores, and optional Grad-CAM heatmaps in real time.

> The model is only useful if it can be queried. Phase 4 turns the checkpoint into a living service that any frontend, hospital system, or downstream pipeline can call over HTTP.

---

## ğŸ“ Final Folder Structure (After Phase 4)

```
kidney-stone-cnn/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py               # FastAPI app â€” 5 endpoints
â”‚   â”œâ”€â”€ inference.py          # KidneyStonePredictor class
â”‚   â””â”€â”€ schemas.py            # Pydantic request/response schemas
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ export_onnx.py        # Export model to ONNX format
â”‚   â””â”€â”€ test_api.py           # Automated end-to-end test suite
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth        # PyTorch checkpoint (Phase 2 output)
â”‚   â””â”€â”€ best_model.onnx       # ONNX export (Phase 4 output)
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ api_test_heatmap.png  # Grad-CAM output saved during test run
â”‚
â”œâ”€â”€ requirements_api.txt      # Minimal deps for serving only
â””â”€â”€ requirements.txt          # Full deps (training + serving)
```

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/predict` | Single image prediction |
| `POST` | `/predict/batch` | Multiple images in one request |
| `GET` | `/health` | Server + model health check |
| `GET` | `/model-info` | Architecture, params, metrics |
| `GET` | `/docs` | Auto-generated Swagger UI |

---

## ğŸ“¤ Request / Response

### `POST /predict`

**Request:** `multipart/form-data`
- `file` â€” JPEG/PNG image
- `include_gradcam` â€” boolean query param (default: `false`)

**Response:**
```json
{
  "prediction": "stone",
  "confidence": 0.9988,
  "probabilities": {
    "stone": 0.9988,
    "no_stone": 0.0012
  },
  "gradcam_heatmap": "<base64-encoded PNG or null>",
  "inference_time_ms": 30.4
}
```

### `POST /predict/batch`

**Request:** Multiple files in `multipart/form-data`

**Response:**
```json
[
  { "filename": "Stone-(817).jpg", "prediction": "stone", "confidence": 0.9988 },
  { "filename": "Normal-(529).jpg", "prediction": "no_stone", "confidence": 0.9757 }
]
```

### `GET /health`
```json
{ "status": "healthy", "device": "mps" }
```

### `GET /model-info`
```json
{
  "architecture": "EfficientNet-B4 + custom classification head",
  "parameters": 18471242,
  "auc": 1.0,
  "sensitivity": 1.0,
  "specificity": 0.9917,
  "f2_score": 0.9877
}
```

---

## âš™ï¸ Implementation Details

### `KidneyStonePredictor` (`api/inference.py`)
- Model loaded **once at startup** via FastAPI lifespan â€” not per request
- Runs on **Apple MPS** (M-series Mac) automatically, falls back to CPU
- Applies identical preprocessing to training: resize 224Ã—224, CLAHE, ImageNet normalisation
- Grad-CAM++ heatmap generation using the final EfficientNet conv layer
- Thread-safe â€” single model instance shared across requests

### Pydantic Schemas (`api/schemas.py`)
- Full type validation on all inputs and outputs
- Invalid file types (non-image uploads) rejected with HTTP 400 before reaching the model

---

## âš¡ ONNX Export

The model was also exported to ONNX for CPU-optimised inference:

```bash
python scripts/export_onnx.py
```

| Runtime | Latency per image |
|---------|-------------------|
| PyTorch (MPS) | 482.6ms |
| ONNX (CPU) | 23.6ms |
| **Speedup** | **20.4Ã—** |

Saved to: `checkpoints/best_model.onnx`

---

## ğŸ§ª Test Results

All 7 tests passed via `scripts/test_api.py`:

| Test | Result |
|------|--------|
| `/health` | âœ… status: healthy, device: mps |
| `/model-info` | âœ… arch + params correct |
| Stone images (3) | âœ… All predicted `stone`, conf 96â€“100% |
| No-stone images (3) | âœ… All predicted `no_stone`, conf 97â€“99% |
| Grad-CAM heatmap | âœ… Generated and saved |
| Batch prediction (4) | âœ… All correct |
| Invalid file type | âœ… Rejected with HTTP 400 |

---

## ğŸš€ How to Start the Server

```bash
cd '/Users/devaguru/Kidney Stone CNN/kidney-stone-cnn'

/Users/devaguru/Kidney\ Stone\ CNN/.venv/bin/uvicorn api.main:app \
  --reload --host 0.0.0.0 --port 8000
```

Then open [http://localhost:8000/docs](http://localhost:8000/docs) for the interactive Swagger UI.

---

## ğŸ§ª How to Run the Test Suite

With the server running in Terminal 1, open Terminal 2:

```bash
cd '/Users/devaguru/Kidney Stone CNN/kidney-stone-cnn'

/Users/devaguru/Kidney\ Stone\ CNN/.venv/bin/python scripts/test_api.py
```

---

## ğŸ” Quick curl Test

```bash
cp "/Users/devaguru/Kidney Stone CNN/kidney-stone-cnn/data/processed/test/stone/Stone- (1004).jpg" /tmp/test_stone.jpg

curl -X POST "http://localhost:8000/predict?include_gradcam=false" \
  -F "file=@/tmp/test_stone.jpg"
```

---

## ğŸ“¦ API Dependencies (`requirements_api.txt`)

```
fastapi>=0.111.0
uvicorn[standard]>=0.29.0
python-multipart>=0.0.9
torch>=2.2.0
torchvision>=0.17.0
timm>=0.9.16
Pillow>=10.0.0
numpy>=1.26.0
opencv-python-headless>=4.9.0
pydantic>=2.0.0
onnxscript
onnx
onnxruntime
```

---

## âš ï¸ Known Limitations

1. **No authentication** â€” The API has no API key or token validation. Do not expose port 8000 publicly without adding auth middleware first.
2. **MPS only on Apple Silicon** â€” The server defaults to MPS on M-series Macs. On Linux/cloud the device falls back to CPU (or CUDA if available).
3. **Grad-CAM adds latency** â€” Heatmap generation adds ~200â€“400ms per request. Keep `include_gradcam=false` for high-throughput use.
4. **No request queuing** â€” Under concurrent load, multiple large batch requests may compete for GPU memory. A task queue (Celery, ARQ) is recommended for production.
5. **ONNX model not yet wired into API** â€” `best_model.onnx` was exported and benchmarked but the API still uses the PyTorch checkpoint. Switching the predictor to ONNX Runtime would give ~20Ã— CPU speedup.

---

## â¡ï¸ Next Phase

**Phase 5 â€” Docker + Deployment:**
- Write `Dockerfile` for the FastAPI app
- `docker build` + `docker run` locally
- Deploy to Railway / Render / EC2

---

*Kidney Stone Detection CNN â€” Internal Research Project*